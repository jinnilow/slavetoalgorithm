# Week 8
## Trial & Error

Following the pseudocode I made, I wanted to explore the different ways I could go about the project. I found a few different resources to help with facial/ emotion tracking. I have found this[ GitHub which explained more about using Face API to track facial expressions](https://github.com/justadudewhohacks/face-api.js), as well as [this one that used the API in ml5 instead of TensorFlow.js.](https://github.com/ml5js/ml5-library/pull/482) 

[This project also shows how he connects visuals according to the most recent emotion tracked.](https://github.com/pseudospencer/emojiCV ) I think this would be really helpful to learn as I figuring how I could connect the visuals to the tracked expressions.

I then stumbled across [The Coding Train’s learning playlist on machine learning,](https://www.youtube.com/playlist?list=PLRqwX-V7Uu6YPSwT06y_AEYTqIwbeam3y ) and this helped me further understand how machine learning works. I have been trying to explore how I could also use machine learning to train my expressions and to connect shapes to the range of expressions instead.

To start understanding machine learning, I was following the image classifying through the webcam tutorial posted by The Coding Train. However I couldn't get the image uploader to work as well as the text to display on the webcam screen instead of just showing it in the backend. 
<br/><br/> 
<img src="https://i.ibb.co/NYb0ggP/Screenshot-2020-09-18-at-1-08-36-PM-01.png" alt="Screenshot-2020-09-18-at-1-08-36-PM-01" border="0">
<br/><br/>


## In-Class Feedback

After talking about my progress and pseudocode, Karen and I were discussing about image classification and how maybe using image could be a better tool for my idea. We were thinking that there could still be a webcam for sure and also maybe an extra step for the users to confirm if the feelings guessed were correct. That could then confirm a final generated visual of the emotional memory?

Although I will have to learn how to include another classification for the images to certain feelings like nostalgic, love, warmth, anxious, anger etc. I do agree that this will actually be a better way of interpreting memories.

I was listening to other people’s feedbacks as well and found that p5.play for animation and three.js for 3D objects would be interesting aspects to further explore. I was also thinking that maybe instead of a webcam, the Text to Speech software could be better to voice out the feelings felt from the memory? 

I might have to rethink my pseudocode to adapt to this new idea.

Hun has also recommended me these few links to check out:
[using 'Vida library, just click the screen and set the static background and then move something] (https://editor.p5js.org/AndreasRef/sketches/8SwQk_ZWN)
<br/><br/> 
<img src="https://i.ibb.co/28rVH8c/Screenshot-2020-09-18-at-1-07-03-PM.png" alt="Screenshot-2020-09-18-at-1-07-03-PM" border="0">
<br/><br/>

and

[colour tracking using colour](http://learningprocessing.com/examples/chp16/example-16-11-ColorTrack
) 

## p5js Tutorial

In this week’s class we learnt how to include a Text To Speech system. With Shuchen’s help, we progressed from laying out the text vertically to a well designed, subtitle-like design.  
<br/><br/> 
<img src="https://i.ibb.co/LC7m6GH/Screenshot-2020-09-18-at-2-50-19-PM.png" alt="Screenshot-2020-09-18-at-2-50-19-PM" border="0" width="800"/>
<br/><br/>
<br/><br/>
<img src="https://i.ibb.co/NFcGXQ4/ezgif-com-gif-maker.gif" alt="ezgif-com-gif-maker" border="0">
<br/><br/>
